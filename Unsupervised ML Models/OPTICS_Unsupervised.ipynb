{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTICS Clustering for Ethereum Fraud Detection\n",
    "\n",
    "This notebook demonstrates unsupervised learning using the OPTICS (Ordering Points To Identify the Clustering Structure) algorithm to detect potential fraudulent activities in Ethereum transactions.\n",
    "\n",
    "## What is OPTICS?\n",
    "OPTICS is a density-based clustering algorithm that works by ordering points to identify the clustering structure. Unlike k-means, OPTICS:\n",
    "- Does not require specifying the number of clusters beforehand\n",
    "- Can find clusters of varying shapes and sizes\n",
    "- Identifies noise points that don't belong to any cluster (potential anomalies)\n",
    "- Handles varying densities better than DBSCAN\n",
    "\n",
    "These characteristics make it particularly suitable for fraud detection, where we don't know in advance how many fraud patterns exist, and fraudulent transactions are often outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import networkx as nx\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Display all columns in DataFrames\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Ethereum Transaction Data\n",
    "\n",
    "Now let's apply OPTICS to Ethereum transaction data. We'll load a dataset containing Ethereum transactions and prepare it for clustering analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 3) (3295031372.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    Let's create visualizations to help understand the clustering results and potential fraud patterns.\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 3)\n"
     ]
    }
   ],
   "source": [
    "# Load sample Ethereum transaction data (replace with your actual data path)\n",
    "# If you don't have data, set SAMPLE_DATA to True to generate synthetic data\n",
    "SAMPLE_DATA = True\n",
    "\n",
    "if not SAMPLE_DATA:\n",
    "    # Load real data - adjust path as needed\n",
    "    try:\n",
    "        transactions_df = pd.read_csv('../sa/eth_transactions.csv')\n",
    "\n",
    "        print(f\"Loaded {len(transactions_df)} real transactions\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Transaction data file not found. Generating synthetic data instead.\")\n",
    "        SAMPLE_DATA = True\n",
    "        \n",
    "if SAMPLE_DATA:\n",
    "    # Generate synthetic data for demonstration\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    n_samples = 1000\n",
    "    n_frauds = 50  # 5% fraudulent transactions\n",
    "    \n",
    "    # Generate random addresses\n",
    "    def random_address():\n",
    "        return '0x' + ''.join([np.random.choice(list('0123456789abcdef')) for _ in range(40)])\n",
    "    \n",
    "    addresses = [random_address() for _ in range(100)]\n",
    "    \n",
    "    # Generate normal transactions\n",
    "    normal_txs = {\n",
    "        'hash': ['0x' + ''.join([np.random.choice(list('0123456789abcdef')) for _ in range(64)]) for _ in range(n_samples - n_frauds)],\n",
    "        'from': [np.random.choice(addresses) for _ in range(n_samples - n_frauds)],\n",
    "        'to': [np.random.choice(addresses) for _ in range(n_samples - n_frauds)],\n",
    "        'value': np.random.pareto(1, n_samples - n_frauds) * 1e17,  # ETH value (in wei)\n",
    "        'gas': np.random.randint(21000, 100000, n_samples - n_frauds),\n",
    "        'gasPrice': np.random.randint(1, 50, n_samples - n_frauds) * 1e9,  # Gas price in wei\n",
    "        'timestamp': np.sort(np.random.randint(1600000000, 1630000000, n_samples - n_frauds)),  # Unix timestamps\n",
    "        'blockNumber': np.random.randint(10000000, 15000000, n_samples - n_frauds)\n",
    "    }\n",
    "    \n",
    "    # Generate fraudulent transactions with anomalous patterns\n",
    "    # Pattern: Very high values, unusual gas prices, concentrated timing\n",
    "    fraud_addresses = [random_address() for _ in range(5)]  # Small group of fraud addresses\n",
    "    fraud_txs = {\n",
    "        'hash': ['0x' + ''.join([np.random.choice(list('0123456789abcdef')) for _ in range(64)]) for _ in range(n_frauds)],\n",
    "        'from': [np.random.choice(fraud_addresses[:2]) for _ in range(n_frauds)],  # Limited senders\n",
    "        'to': [np.random.choice(fraud_addresses[2:]) for _ in range(n_frauds)],    # Limited recipients\n",
    "        'value': np.random.pareto(0.7, n_frauds) * 1e18,  # Much higher values\n",
    "        'gas': np.random.randint(250000, 800000, n_frauds),  # Higher gas values\n",
    "        'gasPrice': np.random.randint(80, 200, n_frauds) * 1e9,  # Unusual gas prices\n",
    "        'timestamp': np.sort(np.random.randint(1615000000, 1615001000, n_frauds)),  # Concentrated in time\n",
    "        'blockNumber': sorted(np.random.randint(12000000, 12001000, n_frauds))\n",
    "    }\n",
    "    \n",
    "    # Combine and create DataFrame\n",
    "    transactions_df = pd.DataFrame({\n",
    "        k: np.concatenate([normal_txs[k], fraud_txs[k]]) for k in normal_txs.keys()\n",
    "    })\n",
    "    \n",
    "    # Add a 'ground_truth' column for evaluation (1=fraud, 0=normal)\n",
    "    # Note: In real-world unsupervised learning, we wouldn't have this\n",
    "    transactions_df['ground_truth'] = [0] * (n_samples - n_frauds) + [1] * n_frauds\n",
    "    \n",
    "    # Shuffle the DataFrame\n",
    "    transactions_df = transactions_df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Generated {len(transactions_df)} synthetic transactions with {n_frauds} fraudulent examples\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "transactions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction for Fraud Detection\n",
    "\n",
    "Now we'll extract relevant features from the transaction data. For fraud detection, we need to consider various aspects like:\n",
    "- Transaction values\n",
    "- Gas usage patterns\n",
    "- Temporal patterns\n",
    "- Network structure features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    \"\"\"Extract features from transaction data for clustering.\"\"\"\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # Transaction value features\n",
    "    features['value_eth'] = df['value'].apply(lambda x: float(x) / 1e18)  # Convert Wei to ETH\n",
    "    \n",
    "    # Gas-related features\n",
    "    features['gas_limit'] = df['gas'].astype(float)\n",
    "    features['gas_price_gwei'] = df['gasPrice'].apply(lambda x: float(x) / 1e9)  # Convert Wei to Gwei\n",
    "    features['total_gas_cost'] = features['gas_limit'] * features['gas_price_gwei'] / 1e9  # In ETH\n",
    "    \n",
    "    # Value/gas ratio (high might indicate washing/unusual transactions)\n",
    "    features['value_gas_ratio'] = features['value_eth'] / (features['total_gas_cost'] + 1e-10)  # Avoid div by zero\n",
    "    \n",
    "    # Time-based features\n",
    "    df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "    features['hour_of_day'] = df['datetime'].dt.hour\n",
    "    features['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "    \n",
    "    # Sort by timestamp and address to calculate time between transactions\n",
    "    df_sorted = df.sort_values(['from', 'timestamp'])\n",
    "    df_sorted['time_diff'] = df_sorted.groupby('from')['timestamp'].diff().fillna(0)\n",
    "    \n",
    "    # Map these time differences back to original DataFrame\n",
    "    time_diff_map = dict(zip(df_sorted.index, df_sorted['time_diff']))\n",
    "    features['time_since_last_tx'] = df.index.map(time_diff_map).fillna(0)\n",
    "    \n",
    "    # Network features\n",
    "    address_counts = {}\n",
    "    for addr in set(df['from'].tolist() + df['to'].tolist()):\n",
    "        address_counts[addr] = {\n",
    "            'sent': len(df[df['from'] == addr]),\n",
    "            'received': len(df[df['to'] == addr])\n",
    "        }\n",
    "    \n",
    "    # Add network features\n",
    "    features['sender_tx_count'] = df['from'].apply(lambda x: address_counts[x]['sent'])\n",
    "    features['recipient_tx_count'] = df['to'].apply(lambda x: address_counts[x]['received'])\n",
    "    features['total_tx_count'] = features['sender_tx_count'] + features['recipient_tx_count']\n",
    "    \n",
    "    # Transaction frequency - to detect sudden bursts\n",
    "    features['tx_frequency'] = features['sender_tx_count'] / (features['time_since_last_tx'] + 1)  # Avoid div by zero\n",
    "    \n",
    "    # Handle extreme values and missing data\n",
    "    # Clip extreme values (beyond 99th percentile)\n",
    "    for col in features.columns:\n",
    "        if features[col].dtype in [np.float64, np.int64]:\n",
    "            upper_limit = features[col].quantile(0.99)\n",
    "            features[col] = features[col].clip(upper=upper_limit)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features from our transaction data\n",
    "transaction_features = extract_features(transactions_df)\n",
    "print(f\"Extracted {transaction_features.shape[1]} features from the transaction data\")\n",
    "\n",
    "# Display feature statistics\n",
    "transaction_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features for better clustering\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(transaction_features)\n",
    "\n",
    "# Convert back to DataFrame for readability\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=transaction_features.columns)\n",
    "scaled_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Running OPTICS Clustering\n",
    "\n",
    "Now we'll apply the OPTICS algorithm to our feature set. OPTICS has several important parameters:\n",
    "\n",
    "- `min_samples`: Number of samples in a neighborhood for a point to be considered a core point\n",
    "- `xi`: Determines the minimum steepness on the reachability plot that constitutes a cluster boundary\n",
    "- `min_cluster_size`: Minimum number of samples in a cluster\n",
    "- `max_eps`: Maximum distance between samples for them to be considered as in the same neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define OPTICS parameters\n",
    "min_samples = 10  # Minimum samples in a neighborhood\n",
    "xi = 0.05         # Steepness threshold for cluster boundary\n",
    "min_cluster_size = max(5, int(0.01 * len(scaled_features)))  # At least 1% of data or 5 points\n",
    "\n",
    "# Create and fit OPTICS model\n",
    "optics_model = OPTICS(\n",
    "    min_samples=min_samples,\n",
    "    xi=xi,\n",
    "    min_cluster_size=min_cluster_size,\n",
    "    metric='euclidean',  # Distance metric\n",
    "    cluster_method='xi'  # Use xi-steep areas for cluster extraction\n",
    ")\n",
    "\n",
    "# Fit the model and get cluster labels\n",
    "cluster_labels = optics_model.fit_predict(scaled_features)\n",
    "\n",
    "# Add cluster labels to our transactions dataframe\n",
    "transactions_df['cluster'] = cluster_labels\n",
    "\n",
    "# Get basic clustering statistics\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = list(cluster_labels).count(-1)\n",
    "\n",
    "print(f\"OPTICS clustering results:\")\n",
    "print(f\"  - Number of clusters: {n_clusters}\")\n",
    "print(f\"  - Number of noise points (potential anomalies): {n_noise} ({n_noise/len(cluster_labels)*100:.2f}%)\")\n",
    "\n",
    "# Count samples in each cluster\n",
    "cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "print(\"\\nCluster sizes:\")\n",
    "for cluster, count in cluster_counts.items():\n",
    "    label = \"Noise (anomalies)\" if cluster == -1 else f\"Cluster {cluster}\"\n",
    "    print(f\"  - {label}: {count} transactions ({count/len(cluster_labels)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing the Clustering Results\n",
    "\n",
    "Let's create visualizations to help understand the clustering results and potential fraud patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: OPTICS Reachability Plot\n",
    "def plot_optics_reachability(optics):\n",
    "    # Get the ordering of points\n",
    "    space = np.arange(len(optics.labels_))\n",
    "    # Get reachability distances\n",
    "    reachability = optics.reachability_[optics.ordering_]\n",
    "    # Get ordered labels\n",
    "    labels = optics.labels_[optics.ordering_]\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Replace infinite values with maximum finite value * 1.1\n",
    "    reach_plot = reachability.copy()\n",
    "    finite_reach = reach_plot[np.isfinite(reach_plot)]\n",
    "    if len(finite_reach) > 0:  # Check if there are any finite values\n",
    "        max_reach = np.max(finite_reach)\n",
    "        reach_plot[~np.isfinite(reach_plot)] = max_reach * 1.1\n",
    "    \n",
    "    # Plot bars\n",
    "    unique_labels = sorted(set(labels))\n",
    "    colors = plt.cm.nipy_spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "    color_map = {label: colors[i] for i, label in enumerate(unique_labels)}\n",
    "    bar_colors = [color_map[label] if label != -1 else 'black' for label in labels]\n",
    "    \n",
    "    plt.bar(space, reach_plot, color=bar_colors, width=1.0)\n",
    "    plt.ylabel('Reachability Distance')\n",
    "    plt.xlabel('Points (ordered by cluster)')\n",
    "    plt.title('OPTICS Reachability Plot')\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [plt.Line2D([0], [0], color='black', lw=4, label='Noise/Anomalies')]\n",
    "    for label in sorted([l for l in unique_labels if l != -1]):\n",
    "        legend_elements.append(plt.Line2D([0], [0], color=color_map[label], lw=4, label=f'Cluster {label}'))\n",
    "    \n",
    "    plt.legend(handles=legend_elements)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualization 2: 2D Projection of Clusters (using t-SNE)\n",
    "def plot_clusters_2d(features, labels, method='tsne'):\n",
    "    # Dimension reduction\n",
    "    if method.lower() == 'tsne':\n",
    "        reducer = TSNE(n_components=2, random_state=42)\n",
    "        title_prefix = 't-SNE'\n",
    "    else:  # default to PCA\n",
    "        reducer = PCA(n_components=2)\n",
    "        title_prefix = 'PCA'\n",
    "    \n",
    "    # Transform data to 2D\n",
    "    features_2d = reducer.fit_transform(features)\n",
    "    \n",
    "    # Create scatter plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Plot regular clusters\n",
    "    unique_labels = sorted(set(labels))\n",
    "    colors = plt.cm.nipy_spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "    \n",
    "    for i, label in enumerate(unique_labels):\n",
    "        if label == -1:\n",
    "            # Plot noise points with black X markers\n",
    "            mask = labels == -1\n",
    "            plt.scatter(features_2d[mask, 0], features_2d[mask, 1], \n",
    "                        marker='x', s=60, color='black', alpha=0.8, label='Noise/Anomalies')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
