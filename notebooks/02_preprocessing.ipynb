{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Ethereum Fraud Detection - Data Preprocessing\n",
       "\n",
       "Author: Victor Oketch Sabare  \n",
       "Date: January 2025\n",
       "\n",
       "This notebook focuses on cleaning and preprocessing the Ethereum transaction data to prepare it for feature engineering and model development."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Setup and Import Libraries"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Data manipulation\n",
       "import pandas as pd\n",
       "import numpy as np\n",
       "\n",
       "# Data preprocessing\n",
       "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
       "from sklearn.impute import SimpleImputer\n",
       "\n",
       "# Visualization\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "\n",
       "# Custom utilities\n",
       "import sys\n",
       "sys.path.append('../')\n",
       "from src.utils.helpers import load_config\n",
       "from src.data.preprocessing import clean_addresses, normalize_values\n",
       "\n",
       "%matplotlib inline"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Load Raw Data"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Load the raw transaction data\n",
       "raw_data = pd.read_csv('../data/raw/ethereum_transactions.csv')\n",
       "print(f\"Loaded {len(raw_data)} transactions\")\n",
       "raw_data.head()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Data Cleaning"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def clean_data(df):\n",
       "    \"\"\"Main cleaning function for transaction data\"\"\"\n",
       "    \n",
       "    # Create copy to avoid modifying original data\n",
       "    cleaned = df.copy()\n",
       "    \n",
       "    # Remove duplicates\n",
       "    cleaned = cleaned.drop_duplicates()\n",
       "    \n",
       "    # Convert timestamps\n",
       "    cleaned['timestamp'] = pd.to_datetime(cleaned['timestamp'], unit='s')\n",
       "    \n",
       "    # Normalize ethereum values to ETH (from Wei)\n",
       "    cleaned['value_eth'] = cleaned['value'] / 1e18\n",
       "    \n",
       "    # Normalize gas prices\n",
       "    cleaned['gas_price_gwei'] = cleaned['gas_price'] / 1e9\n",
       "    \n",
       "    # Handle missing values\n",
       "    cleaned = handle_missing_values(cleaned)\n",
       "    \n",
       "    return cleaned\n",
       "\n",
       "def handle_missing_values(df):\n",
       "    \"\"\"Handle missing values in the dataset\"\"\"\n",
       "    \n",
       "    # Fill numeric missing values with median\n",
       "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
       "    imputer = SimpleImputer(strategy='median')\n",
       "    df[numeric_columns] = imputer.fit_transform(df[numeric_columns])\n",
       "    \n",
       "    return df\n",
       "\n",
       "# Clean the data\n",
       "cleaned_data = clean_data(raw_data)\n",
       "\n",
       "# Display cleaning results\n",
       "print(\"Cleaning Summary:\")\n",
       "print(f\"Original rows: {len(raw_data)}\")\n",
       "print(f\"Cleaned rows: {len(cleaned_data)}\")\n",
       "print(f\"Removed rows: {len(raw_data) - len(cleaned_data)}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. Data Validation"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def validate_data(df):\n",
       "    \"\"\"Validate cleaned data meets requirements\"\"\"\n",
       "    \n",
       "    validations = {\n",
       "        'no_missing_values': df.isnull().sum().sum() == 0,\n",
       "        'valid_timestamps': df['timestamp'].min() > pd.Timestamp('2015-07-30'),  # Ethereum launch date\n",
       "        'valid_values': (df['value_eth'] >= 0).all(),\n",
       "        'valid_gas': (df['gas_price_gwei'] > 0).all()\n",
       "    }\n",
       "    \n",
       "    return pd.Series(validations)\n",
       "\n",
       "# Run validations\n",
       "validation_results = validate_data(cleaned_data)\n",
       "print(\"Validation Results:\")\n",
       "print(validation_results)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. Feature Scaling"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def scale_features(df):\n",
       "    \"\"\"Scale numeric features\"\"\"\n",
       "    \n",
       "    # Select numeric columns for scaling\n",
       "    numeric_features = ['value_eth', 'gas_price_gwei', 'gas_used']\n",
       "    \n",
       "    # Create scalers\n",
       "    standard_scaler = StandardScaler()\n",
       "    minmax_scaler = MinMaxScaler()\n",
       "    \n",
       "    # Apply standard scaling\n",
       "    df_scaled = df.copy()\n",
       "    df_scaled[f'{numeric_features}_scaled'] = standard_scaler.fit_transform(df[numeric_features])\n",
       "    \n",
       "    # Apply minmax scaling\n",
       "    df_scaled[f'{numeric_features}_normalized'] = minmax_scaler.fit_transform(df[numeric_features])\n",
       "    \n",
       "    return df_scaled\n",
       "\n",
       "# Scale the features\n",
       "scaled_data = scale_features(cleaned_data)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 6. Address Normalization"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def normalize_addresses(df):\n",
       "    \"\"\"Normalize Ethereum addresses\"\"\"\n",
       "    \n",
       "    # Convert addresses to lowercase\n",
       "    df['from_address'] = df['from_address'].str.lower()\n",
       "    df['to_address'] = df['to_address'].str.lower()\n",
       "    \n",
       "    # Validate address format\n",
       "    def is_valid_address(addr):\n",
       "        return addr.startswith('0x') and len(addr) == 42\n",
       "    \n",
       "    # Filter valid addresses\n",
       "    valid_from = df['from_address'].apply(is_valid_address)\n",
       "    valid_to = df['to_address'].apply(is_valid_address)\n",
       "    \n",
       "    print(f\"Invalid 'from' addresses: {(~valid_from).sum()}\")\n",
       "    print(f\"Invalid 'to' addresses: {(~valid_to).sum()}\")\n",
       "    \n",
       "    return df[valid_from & valid_to]\n",
       "\n",
       "# Normalize addresses\n",
       "normalized_data = normalize_addresses(scaled_data)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 7. Data Partitioning"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "def partition_data(df):\n",
       "    \"\"\"Partition data by date for temporal analysis\"\"\"\n",
       "    \n",
       "    # Sort by timestamp\n",
       "    df_sorted = df.sort_values('timestamp')\n",
       "    \n",
       "    # Create time-based partitions\n",
       "    partitions = {\n",
       "        'training': df_sorted.iloc[:int(len(df_sorted)*0.7)],\n",
       "        'validation': df_sorted.iloc[int(len(df_sorted)*0.7):int(len(df_sorted)*0.85)],\n",
       "        'testing': df_sorted.iloc[int(len(df_sorted)*0.85):]\n",
       "    }\n",
       "    \n",
       "    return partitions\n",
       "\n",
       "# Create partitions\n",
       "data_partitions = partition_data(normalized_data)\n",
       "\n",
       "# Display partition sizes\n",
       "for name, partition in data_partitions.items():\n",
       "    print(f\"{name}: {len(partition)} records\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 8. Save Processed Data"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Save processed datasets\n",
       "for name, partition in data_partitions.items():\n",
       "    partition.to_csv(f'../data/processed/{name}_data.csv', index=False)\n",
       "    print(f\"Saved {name} dataset\")\n",
       "\n",
       "# Save preprocessing metadata\n",
       "preprocessing_metadata = {\n",
       "    'original_rows': len(raw_data),\n",
       "    'processed_rows': len(normalized_data),\n",
       "    'features': list(normalized_data.columns),\n",
       "    'partition_sizes': {name: len(partition) for name, partition in data_partitions.items()}\n",
       "}\n",
       "\n",
       "import json\n",
       "with open('../data/processed/preprocessing_metadata.json', 'w') as f:\n",
       "    json.dump(preprocessing_metadata, f, indent=2)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 9. Preprocessing Summary"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
       "# Display final preprocessing summary\n",
       "print(\"Preprocessing Summary:\")\n",
       "print(f\"Original records: {len(raw_data)}\")\n",
       "print(f\"After cleaning: {len(cleaned_data)}\")\n",
       "print(f\"After normalization: {len(normalized_data)}\")\n",
       "print(\"\\nFeature Statistics:\")\n",
       "print(normalized_data.describe())"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
    }